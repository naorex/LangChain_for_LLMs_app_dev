{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GXKLhQgofV_S"
      },
      "source": [
        "https://tech.nri-net.com/entry/tried_langchain_to_extend_chatgpt\n",
        "https://python.langchain.com/en/latest/getting_started/getting_started.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xCT-pcNgdgJW"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mKqkOLFCaB4G"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# API key を設定\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ_cG7SOhaA8"
      },
      "source": [
        "# Building a Language Model Application: LLMs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H2wnOJ8rdVXN"
      },
      "source": [
        "## LLMs: Get predictions from a language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d3PLI6aFaNBW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Rainbow Sockery\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.9)\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JQRvQ9Lfdo8G"
      },
      "source": [
        "## Prompt Template: Manage prompts for LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "41p0LJABfU9i"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7xmPeOjweglV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is a good name for a company that makes colorful socks?\n"
          ]
        }
      ],
      "source": [
        "print(prompt.format(product=\"colorful socks\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IeBZFK9Yejrw"
      },
      "source": [
        "## Chains: Combine LLMs and prompts in multi-step workflows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mEWEzkDberbW"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5Hg3rDsRe-bI"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iD5KoLL0e_9q"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nKaleidoscope Socks.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"colorful socks\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iUJu6CfbfRI1"
      },
      "source": [
        "## Agents: Dynamically Call Chains Based on User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ntmjzLjYfij_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Requirement already satisfied: requests in c:\\users\\nao23\\documents\\python_projects\\langchain_for_llms_app_dev_311\\lib\\site-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nao23\\documents\\python_projects\\langchain_for_llms_app_dev_311\\lib\\site-packages (from requests->google-search-results) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nao23\\documents\\python_projects\\langchain_for_llms_app_dev_311\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nao23\\documents\\python_projects\\langchain_for_llms_app_dev_311\\lib\\site-packages (from requests->google-search-results) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nao23\\documents\\python_projects\\langchain_for_llms_app_dev_311\\lib\\site-packages (from requests->google-search-results) (2023.5.7)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (pyproject.toml): started\n",
            "  Building wheel for google-search-results (pyproject.toml): finished with status 'done'\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32076 sha256=23d6479a350f8f0bc0b2c8361e77c34f5dd109f2d6168cdb86ecc71693fd87b5\n",
            "  Stored in directory: c:\\users\\nao23\\appdata\\local\\pip\\cache\\wheels\\6e\\42\\3e\\aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "# SerpAPI Agents\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CU1meX1Nf55j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Rt9sk3prf8ao"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
            "Action: Search\n",
            "Action Input: \"High temperature in SF yesterday\"\u001b[0m"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Got error from SerpAPI: Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools, llm, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# Now let's test it out!\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhat was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\agents\\agent.py:953\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 953\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    954\u001b[0m         name_to_tool_map,\n\u001b[0;32m    955\u001b[0m         color_mapping,\n\u001b[0;32m    956\u001b[0m         inputs,\n\u001b[0;32m    957\u001b[0m         intermediate_steps,\n\u001b[0;32m    958\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    961\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m    962\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    963\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\agents\\agent.py:820\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    818\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    821\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[0;32m    822\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    823\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[0;32m    824\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    825\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:294\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    293\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[0;32m    297\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    298\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:266\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    265\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[0;32m    267\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    268\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:415\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    409\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\n\u001b[0;32m    410\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[0;32m    411\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    412\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    413\u001b[0m     )\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[1;32m--> 415\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    416\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\utilities\\serpapi.py:85\u001b[0m, in \u001b[0;36mSerpAPIWrapper.run\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run query through SerpAPI and parse result.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_response(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresults(query))\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\utilities\\serpapi.py:131\u001b[0m, in \u001b[0;36mSerpAPIWrapper._process_response\u001b[1;34m(res)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Process response from SerpAPI.\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot error from SerpAPI: \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res[\u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    133\u001b[0m     toret \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "\u001b[1;31mValueError\u001b[0m: Got error from SerpAPI: Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key"
          ]
        }
      ],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iQFZ0OAVgX9M"
      },
      "source": [
        "## Memory: Add State to Chains and Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dN2TKyfAgiHI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eEAG01kvhDbL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j5ocRyeUhru0"
      },
      "source": [
        "# Building a Language Model Application: Chat Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "25L4I5pnh0D2"
      },
      "source": [
        "## Get Message Completions from a Chat Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "msASLLp9hzeX"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "udjZA2VSw9vJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SLM0RcDTxBom"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\n",
        "]\n",
        "chat(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KoctD7oPxN0n"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 73, 'completion_tokens': 16, 'total_tokens': 89}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"Translate this sentence from English to French. I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"Translate this sentence from English to French. I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8UU7CGE3xYGW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt_tokens': 73, 'completion_tokens': 16, 'total_tokens': 89}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result.llm_output['token_usage']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A02xlII9xbtn"
      },
      "source": [
        "## Chat Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LcI4etN4xis3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_QLK2qTYxqLv"
      },
      "source": [
        "## Chains with Chat Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HD9hPWIVxt56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"J'adore la programmation.\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf5mxM6Jx135"
      },
      "source": [
        "## Agents with Chat Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPkX_blVyBfW"
      },
      "outputs": [],
      "source": [
        "# SerpAPI Agents\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fWcLP3Lgx4Ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
            "}\n",
            "```\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Got error from SerpAPI: Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools, chat, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mCHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Now let's test it out!\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWho is Olivia Wilde\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms boyfriend? What is his current age raised to the 0.23 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:236\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    235\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 236\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\chains\\base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[0;32m    128\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    129\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    130\u001b[0m     inputs,\n\u001b[0;32m    131\u001b[0m )\n\u001b[0;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 134\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    135\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    136\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    137\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\agents\\agent.py:953\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 953\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    954\u001b[0m         name_to_tool_map,\n\u001b[0;32m    955\u001b[0m         color_mapping,\n\u001b[0;32m    956\u001b[0m         inputs,\n\u001b[0;32m    957\u001b[0m         intermediate_steps,\n\u001b[0;32m    958\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m    959\u001b[0m     )\n\u001b[0;32m    960\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    961\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[0;32m    962\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[0;32m    963\u001b[0m         )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\agents\\agent.py:820\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[0;32m    818\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    821\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[0;32m    822\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    823\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[0;32m    824\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    825\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[0;32m    826\u001b[0m     )\n\u001b[0;32m    827\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:294\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    293\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[1;32m--> 294\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    295\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    296\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[0;32m    297\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    298\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:266\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    265\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 266\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[0;32m    267\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    268\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[0;32m    269\u001b[0m     )\n\u001b[0;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\tools\\base.py:415\u001b[0m, in \u001b[0;36mTool._run\u001b[1;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Use the tool.\"\"\"\u001b[39;00m\n\u001b[0;32m    407\u001b[0m new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    409\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\n\u001b[0;32m    410\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[0;32m    411\u001b[0m         callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    412\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    413\u001b[0m     )\n\u001b[0;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[1;32m--> 415\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    416\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\utilities\\serpapi.py:85\u001b[0m, in \u001b[0;36mSerpAPIWrapper.run\u001b[1;34m(self, query, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run query through SerpAPI and parse result.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_response(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresults(query))\n",
            "File \u001b[1;32mc:\\Users\\nao23\\Documents\\Python_Projects\\LangChain_for_LLMs_app_dev_311\\Lib\\site-packages\\langchain\\utilities\\serpapi.py:131\u001b[0m, in \u001b[0;36mSerpAPIWrapper._process_response\u001b[1;34m(res)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Process response from SerpAPI.\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot error from SerpAPI: \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res[\u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    133\u001b[0m     toret \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "\u001b[1;31mValueError\u001b[0m: Got error from SerpAPI: Invalid API key. Your API key should be here: https://serpapi.com/manage-api-key"
          ]
        }
      ],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_aGUh9cpyEfX"
      },
      "source": [
        "## Memory: Add State to Chains and Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qauzIkVeyZD8"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate, \n",
        "    MessagesPlaceholder, \n",
        "    SystemMessagePromptTemplate, \n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")\n",
        "\n",
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "\n",
        "conversation.predict(input=\"Tell me about yourself.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xCT-pcNgdgJW"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
